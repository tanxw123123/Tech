## 1.Kafka消息队列

### 1.消息队列两种模式

- 点对点：Queue，不可重复消费
- 发布订阅：Topic，可以重复消费

1.点对点

```
    消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。
```

2.发布订阅

```
    消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。
```

---



### 2.入门

#### 1.consumer 是推还是拉？

```shell
# customer 应该从 brokes 拉取消息还是 brokers 将消息推送到 consumer?
- producer 将消息推送到 broker，consumer 从broker 拉取消息.

# 为什么不用broker推送到consumer？
- 由 broker 决定消息推送的速率
- 消息系统都致力于让 consumer 以最大的速率最快速的消费消息
- push 模式下，当 broker 推送的速率远大于 consumer 消费的速率时，consumer 恐怕就要崩溃了

# pull的缺点?
- 如果 broker 没有可供消费的消息，将导致 consumer 不断在循环中轮询，直到新消息到达。
- 为了避免这点，Kafka 有个参数可以让 consumer阻塞直到新消息到达(当然也可以阻塞直到消息的数量达到某个特定的量这样就可以批量发送)。
```

#### 2.维护消费状态跟踪的方法

```shell
# 大部分消息系统
- 一个消息被分发到consumer 后 broker 就马上进行标记或者等待 customer 的通知后进行标记。这样也可以在消息在消费后立马就删除以减少空间占用。

- 如果一条消息发送出去之后就立即被标记为消费过的，但是 consumer 处理消息时失败了（比如程序崩溃）消息就丢失了。
- 为了解决这个问题，很多消息系统提供了另外一个功能：
  当消息被发送出去之后仅仅被标记为已发送状态，当接到 consumer 已经消费成功的通知后才标记为已被消费的状态
  
# 首先如果 consumer处理消息成功了但是向 broker 发送响应时失败了，这条消息将被消费两次。
Kafka 采用了不同的策略：
  - Topic 被分成了若干分区，每个分区在同一时间只被一个 consumer 消费。这意味着每个分区被消费的消息在日志中的位置仅仅是一个简单的整数：offset。
  - 这样就很容易标记每个分区消费状态就很容易了，仅仅需要一个整数而已。这样消费状态的跟踪就很简单了。
```

#### 3.消费组

```
在 Kafka 的消费理念中还有一层消费组的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。
```

![image-20230125174232848](D:\Tech\linux\System\assets\image-20230125174232848.png)

```
我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力
但是，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区！
```

#### 4.活锁问题

```shell
# 消费者故障，出现活锁问题如何解决？

- kafka 消费者正常情况下是订阅一个 topic 并且能够 poll 消息。该消费者会占用一个分区，同时需要定时向 zk 发送心跳监测，以证明自己活着。当消费者占用一个分区后，且能够正常发送心跳，但是不 poll 消息了，不再进行消息处理了，这种情况下就出现了活锁。

- kafka 这边处理的时候会配置 max.poll.interval.ms 活跃监测机制。如果客户端调用 poll 的频率大于最大间隔，就会将当前客户端连接断开，让其它的消费者过来消费。
```

#### 5.数据重复消费问题

```shell
https://zhuanlan.zhihu.com/p/366931355
- 这些问题通常不是 MQ 自己保证的，是由开发来保证的
- kafka 有个offset的概念，就是每写进去一个消息，都有自己的一个offset，代表消息的序号，然后consumer消费了消息之后，每隔一段时间（kafka可以配置），会把自己消费过的消息的offset提交一下，表示“我已经消费过了，下一次重启什么的，就接着这个offset继续“。

# 举个重复消费的例子：
有这么个场景。数据 1/2/3 依次进入 Kafka，我们就假设分配的 offset 依次是 152/153/154。
消费者从 Kafka 去消费的时候，也是按照这个顺序去消费。
假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 Zookeeper，此时消费者进程被重启了。
那么此时消费过的数据 1/2 的 offset 并没有提交，Kafka 也就不知道你已经消费了 offset=153 这条数据。
那么重启之后，消费者会找 Kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。
由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。

【注意】新版的 Kafka 已经将 offset 的存储从 Zookeeper 转移至 Kafka brokers，并使用内部位移主题 __consumer_offsets 进行存储。
```

![image-20230126091430989](D:\Tech\linux\System\assets\image-20230126091430989.png)



```shell
# 那么如何解决重复消费的问题呢？
其实重复消费不可怕，可怕的是没有考虑重复消费之后怎么保证幂等性

这个需要结合实际实际业务考虑，举例:

- 比如，拿到数据需要写入数据库，是不是得先判断一下，这个数据在数据库中存不存在，如果已经有了，update一下就可以了。
- 在比如，拿到数据是写redis，那直接set就可以了。天然幂等性。
- 再比如 稍微复杂点，让生产者发送每条数据的时候，添加一个全局唯一的id，然后消费的时候，先根据这个id去查有没有这个消息，再做逻辑处理。
```



#### 6.如何保证消费的顺序性？

```shell
    我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。

- 消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。
- 接着，我们在消费者里可能会搞多个线程来并发处理消息。
- 因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。
- 而多个线程并发跑的话，顺序可能就乱掉了。
```

如图：

![image-20230126100820954](D:\Tech\linux\System\assets\image-20230126100820954.png)



**解决方法：**写N个queue，将具有相同key的数据都存储到同一个queue，然后对N个线程，每个线程分别消费一个queue即可

![image-20230126101403339](D:\Tech\linux\System\assets\image-20230126101403339.png)



#### 7.kafka与传统MQ的区别



```shell
(1) Kafka 持久化日志，这些日志可以被重复读取和无限期保留
(2) Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性
(3) Kafka 支持实时的流式处理
```



#### 8.Kafka 创建 Topic 时如何将分区放置到不同的 Broker 中

```
如果我们有 5 个Broker，5 个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五
个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个Broker 上，依次类推
```



#### 9.Kafka 新建的分区会在哪个目录下创建

```shell
`在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，
`这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。

1. 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个
目录下创建文件夹用于存放数据

2. 但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区 ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录
```



#### 10.producer 是否直接将数据发送到 broker 的 leader(主节点)？

```shell
`producer 直接将数据发送到 broker 的 leader(主节点)

不需要在多个节点进行分发，为了帮助 producer 做到这点，
所有的 Kafka 节点都可以及时的告知:哪些节点是活动的，目标 topic 目标分区的 leader 在哪
```



#### 11.简述kafka的ack机制

```shell
`Kafka的ack机制，指的是producer的消息发送确认机制，这直接影响到Kafka集群的吞吐量和消息可靠性。而吞吐量和可靠性就像硬币的两面，两者不可兼得，只能平衡。

- ack有3个可选值，分别是1，0，-1。

1.  ack=1，简单来说就是，producer只要收到一个分区副本成功写入的通知就认为推送消息成功了。这里有一个地方需要注意，这个副本必须是leader副本。只有leader副本成功写入了，producer才会认为消息发送成功。

注意，ack的默认值就是1。这个默认值其实就是吞吐量与可靠性的一个折中方案。生产上我们可以根据实际情况进行调整，比如如果你要追求高吞吐量，那么就要放弃可靠性。

2.  ack=0，简单来说就是，producer发送一次就不再发送了，不管是否发送成功。

3.  ack=-1，简单来说就是，producer只有收到分区内所有副本的成功写入的通知才认为推送消息成功了。



-------------------------------------------------------------------------------------
接下来我们分析一下ack=1的情况下，为什么消息也会丢失？

ack=1的情况下，producer只要收到分区leader成功写入的通知就会认为消息发送成功了。如果leader成功写入后，还没来得及把数据同步到follower节点就挂了，这时候消息就丢失了。
```



#### 12.kafaka 生产数据时数据的分组策略

```shell
生产者决定数据产生到集群的哪个 partition 中

每一条消息都是以（key，value）格式

Key 是由生产者发送数据传入

所以生产者（key）决定了数据产生到集群的哪个 partition
```



---



**概念**

（1）生产者和消费者（producer和consumer）：消息的发送者叫 Producer，消息的使用者和接受者是 Consumer，生产者将数据保存到 Kafka 集群中，消费者从中获取消息进行业务的处理。

（2）broker：Kafka 集群中有很多台 Server，其中每一台 Server 都可以存储消息，将每一台 Server 称为一个 kafka 实例，也叫做 broker。

（3）主题（topic）：一个 topic 里保存的是同一类消息，相当于对消息的分类，每个 producer 将消息发送到 kafka 中，都需要指明要存的 topic 是哪个，也就是指明这个消息属于哪一类。

（4）分区（partition）：每个 topic 都可以分成多个 partition，每个 partition 在存储层面是 append log 文件。任何发布到此 partition 的消息都会被直接追加到 log 文件的尾部。为什么要进行分区呢？最根本的原因就是：kafka基于文件进行存储，当文件内容大到一定程度时，很容易达到单个磁盘的上限，因此，采用分区的办法，一个分区对应一个文件，这样就可以将数据分别存储到不同的server上去，另外这样做也可以负载均衡，容纳更多的消费者。

（5）偏移量（Offset）：一个分区对应一个磁盘上的文件，而消息在文件中的位置就称为 offset（偏移量），offset 为一个 long 型数字，它可以唯一标记一条消息。由于kafka 并没有提供其他额外的索引机制来存储 offset，文件只能顺序的读写，所以在kafka中几乎不允许对消息进行“随机读写”。

**综上，我们总结一下 Kafka 的几个要点:**

- kafka 是一个基于发布-订阅的分布式消息系统（消息队列）
- Kafka 面向大数据，消息保存在主题中，而每个 topic 有分为多个分区
- kafka 的消息数据保存在磁盘，每个 partition 对应磁盘上的一个文件，消息写入就是简单的文件追加，文件可以在集群内复制备份以防丢失
- 即使消息被消费，kafka 也不会立即删除该消息，可以通过配置使得过一段时间后自动删除以释放磁盘空间
- kafka依赖分布式协调服务Zookeeper，适合离线/在线信息的消费，与 storm 和 spark 等实时流式数据分析常常结合使用



**副本（replicated）**

```
kafka 还可以配置 partitions 需要备份的个数(replicas),每个 partition 将会被备份到多台机器上,以提高可用性，备份的数量可以通过配置文件指定。

  这种冗余备份的方式在分布式系统中是很常见的，那么既然有副本，就涉及到对同一个文件的多个备份如何进行管理和调度。kafka 采取的方案是：每个 partition 选举一个 server 作为“leader”，由 leader 负责所有对该分区的读写，其他 server 作为 follower 只需要简单的与 leader 同步，保持跟进即可。如果原来的 leader 失效，会重新选举由其他的 follower 来成为新的 leader。

  至于如何选取 leader，实际上如果我们了解 ZooKeeper，就会发现其实这正是 Zookeeper 所擅长的，Kafka 使用 ZK 在 Broker 中选出一个 Controller，用于 Partition 分配和 Leader 选举。

  另外，这里我们可以看到，实际上作为 leader 的 server 承担了该分区所有的读写请求，因此其压力是比较大的，从整体考虑，有多少个 partition 就意味着会有多少个leader，kafka 会将 leader 分散到不同的 broker 上，确保整体的负载均衡。
```

---

### 3.安装

#### 1.安装zookeeper

https://www.w3cschool.cn/apache_kafka/apache_kafka_installation_steps.html

```shell
$ wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz
$ tar -xf zookeeper-3.4.6.tar.gz
$ cd zookeeper-3.4.6
$ mkdir data

$ vi conf/zoo.cfg
tickTime=2000
dataDir=/path/to/zookeeper/data
clientPort=2181
initLimit=5
syncLimit=2
```

启动zookeeper：

```shell
$ bin/zkServer.sh start  # 启动
$ bin/zkServer.sh stop   # 停止
```

#### 2.安装Kafka



```shell
$ wget https://archive.apache.org/dist/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz
$ tar -xf kafka_2.11-0.9.0.0.tgz
$ cd kafka_2.11-0.9.0.0
$ vim config/server.properties
listeners=PLAINTEXT://192.168.254.51:9092
```

启动Kafka：

```shell
$ bin/kafka-server-start.sh -daemon config/server.properties      # 启动kafka命令加上–daemon，那么kafka会以守护进程的方式启动
$ bin/kafka-server-stop.sh config/server.properties   # 停止

```

#### 3.配置

1.创建队列

- --zookeeper指定zookeeper的地址和端口
- --partitions指定partition的数量
- --replication-factor指定数据副本的数量

```shell
$ kafka-topics.sh --create --zookeeper 192.168.254.51:2181 --topic kb09demo --partitions 1 --replication-factor 1

//创建Topic  5个分区
$ kafka-topics.sh --create --zookeeper 192.168.254.51:2181 --topic kb09demo2 --partitions 5 --replication-factor 1
```

2.查看队列列表

```shell
$ kafka-topics.sh --zookeeper 192.168.254.51:2181 --list
```

3.查看指定topic明细

```shell
$ kafka-topics.sh --zookeeper 192.168.254.51:2181 --topic kb09demo --describe
$ kafka-topics.sh --zookeeper 192.168.254.51:2181 --topic kb09demo2 --describe
```

4.修改列对

```shell
# 下面命令，增加partion数量，从5个partition增加到6个
$ kafka-topics.sh --zookeeper 192.168.254.51:2181 --alter --topic kb09demo2 --partitions 6

# 但是减少partition是不允许的。如果执行配置的partition变少，会抛出一个错误，显示partition数量只能增加
```

5.删除队列

```shell
# 删除之前，需要先将server.properties文件中的配置delete.topic.enable=true更改一下，否则执行删除命令不会生效。
$ kafka-topics.sh --zookeeper 192.168.254.51:2181 --delete --topic kb09demo
```

6.Producer（消息的生成者）

创建生产消息
运行Producer并将Terminal输入的消息发送到服务端

```shell
$ kafka-console-producer.sh --topic kb09demo --broker-list 192.168.254.51:9092
```

7.Consumer（消息的消费者）

启动Consumer读取消息并输出到标准输出

```shell
$ kafka-console-consumer.sh --zookeeper 192.168.254.51:2181 --topic kb09demo2 --bootstrap-server 192.168.254.51:9092 --from-beginning

# 如果加上from-beginning指定从第一条数据开始消费
# --bootstrap-server 192.168.254.51:9092 此参数为可选项
```

---

### 4.kafka集群迁移

**1.先迁生产者，再迁消费者**

​        指先将生产消息的业务迁移到新的Kafka，原Kafka不会有新的消息生产。待原有Kafka实例的消息全部消费完成后，再将消费消息业务迁移到新的Kafka，开始消费新Kafka实例的消息。

1. 将生产客户端的Kafka连接地址修改为新Kafka实例的连接地址。
2. 重启生产业务，使得生产者将新的消息发送到新Kafka实例中。
3. 观察各消费组在原Kafka的消费进度，直到原Kafka中数据都已经被消费完毕。
4. 将消费客户端的Kafka连接地址修改为新Kafka实例的连接地址。
5. 重启消费业务，使得消费者从新Kafka实例中消费消息。
6. 观察消费者是否能正常从新Kafka实例中获取数据。
7. 迁移结束。

​         本方案为业界通用的迁移方案，操作步骤简单，迁移过程由业务侧自主控制，整个过程中消息不会存在乱序问题，**适用于对消息顺序有要求的场景**。但是该方案中需要等待消费者业务直至消费完毕，存在一个时间差的问题，部分数据可能存在较大的端到端时延。

```shell
#如何查看kafka消息消费进度以及是否有未消费的消息
# 查看消费组列表
$ kafka-consumer-groups.sh --bootstrap-server <kafka-ip>:9092 --list
# 查看 kafka 中某一个消费者组的消费情况
$ kafka-consumer-groups.sh --bootstrap-server <kafka-ip>:9092 --group test-17 --describe
GROUP           TOPIC                    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
logstash        wsdy-prod-nginx-info     0          20117           20117           0               logstash-0-6dd159ce-f977-4efc-b99c-20d2f5b0f848 /192.168.254.60 logstash-0
logstash        wsdy-prod-ubuntu01-info  0          12111           12111           0               logstash-0-6dd159ce-f977-4efc-b99c-20d2f5b0f848 /192.168.254.60 logstash-0
logstash        wsdy-prod-ubuntu01-error 0          3               3               0               logstash-0-6dd159ce-f977-4efc-b99c-20d2f5b0f848 /192.168.254.60 logstash-0

---
在上面这张图中，我们可以看到该消费者组消费的 topic、partition、当前消费到的 offset 、最新 offset 、LAG(消费进度) 等等。如果消费者的 offset 很长时间没有提交导致 LAG 越来越大，则证明消费 Kafka 的服务异常。

消费者组消费 topic 的元数据信息，在旧版本里面是存储在 zookeeper 中，但由于 zookeeper 并不适合大批量的频繁写入操作，新版 kafka 已将消费者组的元数据信息保存在 kafka 内部的 topic 中，即 __consumer_offsets ，并提供了 kafka-console-consumer.sh 脚本供用户查看消费者组的元数据信息。
```

---



## 2.ActiveMQ

### 1.安装

#### 1.单机（ubuntu）

```shell
$ apt update
$ apt install openjdk-11-jre -y    # 安装java
$ java -version
```

```shell
# 安装
$ mkdir /data && cd /data
$ wget https://archive.apache.org/dist/activemq/5.16.3/apache-activemq-5.16.3-bin.tar.gz
$ tar -xf apache-activemq-5.16.3-bin.tar.gz
$ mv apache-activemq-5.16.3 activemq
```

```shell
创建群组帐户 activemq 运行 Apache ActiveMQ。
$ sudo addgroup --quiet --system activemq     # 创建组
$ sudo adduser --quiet --system --ingroup activemq --no-create-home --disabled-password activemq    # 创建用户
$ chown -R activemq:activemq activemq
```

创建一个 ActiveMQ systemd 服务文件来控制 Apache ActiveMQ 服务:

```shell
$ sudo nano /etc/systemd/system/activemq.service

[Unit]
Description=Apache ActiveMQ
After=network.target
[Service]
Type=forking
User=activemq
Group=activemq

ExecStart=/data/activemq/bin/activemq start
ExecStop=/data/activemq/bin/activemq stop

[Install]
WantedBy=multi-user.target
```

编辑 `jetty.xml` 配置文件来更改监听主机：

```
$ vim /data/activemq/conf/jetty.xml

<property name="host" value="127.0.0.1"/>  将其更改为：
<property name="host" value="0.0.0.0"/>
```

```
$ sudo systemctl daemon-reload
$ sudo systemctl start activemq
$ sudo systemctl enable activemq

```

访问：https://192.168.x.x:8161/admin/

账密在： /data/activemq/conf/users.properties



**测试logstash日志输出到ActiveMQ:**

```shell
# 安装logstash
$ yum -y install java-11-openjdk java-11-openjdk-devel     #首先安装java环境
$ wget https://artifacts.elastic.co/downloads/logstash/logstash-7.10.1-x86_64.rpm
$ rpm -ivh logstash-7.10.1-x86_64.rpm

# 安装stomp插件
$ cd /opt 
$ git clone https://github.com/logstash-plugins/logstash-output-stomp.git
$ yum -y install ruby ruby-devel rubygems    # 安装gem
$ cd logstash-output-stomp/
$ gem build logstash-output-stomp.gemspec    # 编译插件
$ vim /usr/share/logstash/Gemfile       # 添加下面这行代码
gem "logstash-output-stomp", :path => "/opt/logstash-output-stomp"
$ /usr/share/logstash/bin/logstash-plugin install --no-verify
$ /usr/share/logstash/bin/logstash-plugin install logstash-output-stomp

# 安装fileter插件
$ /usr/share/logstash/bin/logstash-plugin install logstash-filter-multiline

------------------------------------------
# 配置文件
$ vim /etc/logstash/conf.d/logstash.conf

input {      # filebeat发送到logstash
  beats {
    codec => plain{ charset => "UTF-8" }
    port => 5044
  }
}

filter {
    multiline {
        pattern => "^\d"
        negate => true
        what => "previous"
        }
}

output {      #输出到ActiveMQ
  stomp {
    host => "192.168.254.188"     # ActiveMQ地址
    port => "61613"
    destination => "/topic/test100"     # 输出到主题
    user => "admin"
    password => "admin"
  }
  stdout {
      codec => json_lines
  }
}
```

```shell
$ systemctl start logstash
$ ss -tlnp|grep 5044
```

日志通过logstash输出到ActiveMQ，可以看到

![image-20230114190520671](D:\Tech\linux\System\assets\image-20230114190520671.png)

消息已经成功发到消息队列，接下来就是消费消息了！！



**python发送接收消息：**

```shell
# 发送消息
> pip3 install stomp.py

代码如下：

import stomp
import time

topic_name = '/topic/test100'
conn = stomp.Connection([('192.168.254.188', 61613)])
conn.connect(username='admin', passcode='admin', wait=True)

def send_to_topic(msg):
    print('-------------消息发送--------------')
    conn.send(body=str(msg), destination=topic_name)
    print(msg)


if __name__ == "__main__":
    send_to_topic("len789")
    conn.disconnect()
```



```shell
# 接收消息
import stomp

topic_name = '/topic/test100'
conn = stomp.Connection([('192.168.254.188',61613)])
conn.connect(username='admin', passcode='admin', wait=True)

class SampleListener(object):
    def on_message(self, headers, message):
        print('headers: %s' % headers)
        print('message: %s' % message)

def receive_from_queue():
    conn.set_listener('SampleListener', SampleListener())
    conn.subscribe(topic_name, 12)
    while True:
        pass

if __name__ == "__main__":
    receive_from_queue()
    conn.disconnect()
```



#### 2.docker安装ActiveMQ



**2.1 安装ActiveMQ**

```shell
$ docker run --name activemq -d -p 61616:61616 -p 8161:8161 -p 61613:61613 rmohr/activemq
```



**2.2 logstash输出到ActiveMQ**

```shell
$ docker run -d --name=logstash logstash:7.14.0

# 进入容器安装插件
$ docker exec -it 容器ID /bin/bash
bash-4.2$ bin/logstash-plugin install logstash-output-stomp
bash-4.2$ bin/logstash-plugin install logstash-filter-multiline

$ docker cp logstash:/usr/share/logstash /data/logstash       # 将目标拷贝到本地
$ chmod 777 -R /data/logstash/
$ mkdir /data/logstash/config/conf.d
```

```shell
$ vim /data/logstash/config/conf.d/logstash.conf

input {      # filebeat发送到logstash
  beats {
    codec => plain{ charset => "UTF-8" }
    port => 5044
  }
}

filter {
    multiline {
        pattern => "^\d"
        negate => true
        what => "previous"
        }
}

output {      #输出到ActiveMQ
  stomp {
    host => "172.17.0.3"     # ActiveMQ地址
    port => "61613"
    destination => "/topic/test100"     # 输出到主题
    user => "admin"
    password => "admin"
  }
  stdout {
      codec => json_lines
  }
}

```

```shell
# 配置conf
$ vim /data/logstash/config/pipelines.yml
- pipeline.id: main
  path.config: "/usr/share/logstash/config/conf.d/logstash.conf"   

# path也是在docker里的绝对路径
```

```shell
配置java
$ vim /data/logstash/config/jvm.options
-Xms512m
-Xmx512m
```

```shell
$ vim /data/logstash/config/logstash.yml
http.host: "0.0.0.0"
#xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]   
将连接es这行注释
```

```shell
# 启动
$ docker run -d --name=logstash -v /data/logstash:/usr/share/logstash -p 5044:5044 logstash:7.14.0
```



### 2.作用及原理

- 系统之间进行通信
- 系统之间的调用进行解耦/异步通信

### 3.通信方式

1.发布-订阅

```shell
- 多接收客户端的方式
- 多个接收客户端

接收消息时有两种方法：
    - destination的receive方法
    - message listener 接口的onMessage 方法
    
- 特点：数据容易丢失
```

2.点对点

```shell
- 独享一条通信链路
- 一方发送消息，另外一方接收

和前面pub-sub的区别:
    - 一个topic有一个发送者和多个接收者
    - p2p里一个queue只有一个发送者和一个接收者。
    
- 特点：能够保证数据安全 
```

